{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q-1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Ans- Information Gain (IG) measures the reduction in impurity after a split.\n",
        "\n",
        "It is calculated as:\n",
        "\n",
        "[ IG = H(Parent) - \\sum_{j} \\frac{N_j}{N} H(Child_j) ]\n",
        "\n",
        "where:\n",
        "\n",
        "( H(Parent) ) is the entropy of the parent node.\n",
        "( H(Child_j) ) is the entropy of the child nodes.\n",
        "( N_j ) is the number of instances in child ( j ).\n",
        "( N ) is the total number of instances."
      ],
      "metadata": {
        "id": "JrGF92GMlXOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-2. What is the difference between Gini Impurity and Entropy?\n",
        "| Measure | Gini Impurity | Entropy |\n",
        "|---------|--------------|---------|\n",
        "| Formula | \\( 1 - \\sum p_i^2 \\) | \\( -\\sum p_i \\log_2 p_i \\) |\n",
        "| Interpretation | Measures misclassification probability | Measures dataset uncertainty |\n",
        "| Computation | Faster | Slower |\n"
      ],
      "metadata": {
        "id": "DLUyxVO5mSTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_So8si2CmUUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-3. What is Pre-Pruning in Decision Trees?  \n",
        "- **Pre-Pruning** stops the tree from growing too deep.  \n",
        "- It uses **stopping criteria** like:  \n",
        "  - Minimum number of samples per node.  \n",
        "  - Maximum tree depth.  \n",
        "  - Minimum impurity decrease."
      ],
      "metadata": {
        "id": "99u0YF_rmbjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical question\n",
        "'''\n",
        "Q4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n",
        "'''\n",
        "'''\n",
        "Answer:-4\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Loading the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Spliting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training Decision Tree with Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ7kyq-8mxzX",
        "outputId": "acaa1124-e491-4ce7-ed0d-2dedf3f4c863"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.         0.01667014 0.40593501 0.57739485]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification, regression, and outlier detection tasks. It works by finding the hyperplane that best divides a dataset into classes. The optimal hyperplane is determined by maximizing the margin between the classes, which is the distance between the hyperplane and the nearest data points from each class, known as support vectors.\n",
        "Example: If you have a dataset of emails labeled as \"spam\" or \"not spam,\" an SVM can be trained to classify new emails into these categories by finding the best separating hyperplane between the two classes."
      ],
      "metadata": {
        "id": "sO7suuI5m-Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-6. What is the Kernel Trick in SVM?\n",
        "\n",
        "The Kernel Trick is a technique used in SVM to handle non-linear classification and regression problems. It involves transforming the input data into a higher-dimensional feature space using a kernel function, where a linear separator can be found. The kernel function computes the inner product of the transformed features without explicitly performing the transformation.\n",
        "\n",
        "Common kernel functions include:\n",
        "\n",
        "Linear Kernel\n",
        "Polynomial Kernel\n",
        "Radial Basis Function (RBF) Kernel\n",
        "Example: In a dataset where the classes are not linearly separable in the original feature space, the Kernel Trick can be used to map the data into a higher-dimensional space where a linear hyperplane can separate the classes."
      ],
      "metadata": {
        "id": "pHZ6YxwQnHS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical question\n",
        "'''\n",
        "Q-7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "'''\n",
        "'''\n",
        "Answer:-7\n",
        "'''\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f'Linear Kernel Accuracy: {accuracy_linear:.2f}')\n",
        "print(f'RBF Kernel Accuracy: {accuracy_rbf:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBlU_hDZnXGU",
        "outputId": "c1e1399a-fa2f-4b42-9ffe-e27f42fc6a66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.00\n",
            "RBF Kernel Accuracy: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-8.  What is the Naive Bayes classifier, and why is it called \"Naive\"?\n",
        "\n",
        "The Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It is used for classification tasks and assumes that the features are conditionally independent given the class label. The \"naive\" assumption of feature independence simplifies the computation, making the algorithm efficient and easy to implement.\n",
        "Example: In text classification, the Naive Bayes classifier can be used to classify emails as \"spam\" or \"not spam\" by calculating the probability of each class given the words in the email and assuming that the presence of each word is independent of the others."
      ],
      "metadata": {
        "id": "HpOLyz_lnhge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-9. Explain the differences between Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n",
        "\n",
        "Gaussian Naive Bayes:\n",
        "\n",
        "Assumes that the features follow a normal (Gaussian) distribution.\n",
        "Suitable for continuous data.\n",
        "Multinomial Naive Bayes:\n",
        "\n",
        "Assumes that the features follow a multinomial distribution.\n",
        "Suitable for discrete data, such as word counts in text classification.\n",
        "Bernoulli Naive Bayes:\n",
        "\n",
        "Assumes that the features follow a Bernoulli distribution (binary/boolean values).\n",
        "Suitable for binary data, such as the presence or absence of words in text classification.\n",
        "Example: Gaussian Naive Bayes can be used for classifying Iris flowers based on continuous features like petal length and width. Multinomial Naive Bayes can be used for classifying documents based on word counts. Bernoulli Naive Bayes can be used for classifying documents based on the presence or absence of specific words."
      ],
      "metadata": {
        "id": "mLKE6wpFnrgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Practical question\n",
        "'''\n",
        "Q10. Write a Python program to train a Gaussian Naive Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "'''\n",
        "'''\n",
        "Answer:-10\n",
        "'''\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOwGWF8Kn-4z",
        "outputId": "358cc65f-883b-4e4c-8645-f28cb3018c33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.97\n"
          ]
        }
      ]
    }
  ]
}