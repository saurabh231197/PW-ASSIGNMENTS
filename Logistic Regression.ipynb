{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXotsGfu3KVv"
      },
      "source": [
        "# Regression Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t-db6YK3KVz"
      },
      "source": [
        "Q1. What is Simple Linear Regression?  \n",
        "- Simple Linear Regression is a statistical method that models the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. The model assumes a linear relationship between the two variables, represented by the equation:\n",
        "\n",
        "`[ Y = mX + c ]`\n",
        "\n",
        "where:\n",
        "- \\( Y \\) is the dependent variable.\n",
        "- \\( X \\) is the independent variable.\n",
        "- \\( m \\) is the slope of the line (the change in \\( Y \\) for a one-unit change in \\( X \\)).\n",
        "- \\( c \\) is the intercept (the value of \\( Y \\) when \\( X \\) is zero).\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q2. What are the key assumptions of Simple Linear Regression?  \n",
        "- **Linearity:** The relationship between the independent and dependent variables should be linear.  \n",
        "- **Independence:** The residuals (errors) should be independent.  \n",
        "- **Homoscedasticity:** The residuals should have constant variance at every level of the independent variable.  \n",
        "- **Normality:** The residuals should be normally distributed.  \n",
        "\n",
        "**Example:** If the residuals show a pattern or funnel shape, it indicates a violation of homoscedasticity.  \n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q3. : Write the mathematical equation for a simple linear regression model and\n",
        "explain each term.\n",
        "- the mathematical equation for a simple linear regression model:\n",
        "y = \\beta_0 + \\beta_1 x + \\varepsilon\n",
        "\n",
        "- y: The dependent variable (or response variable) — the outcome you're trying to predict or explain.\n",
        "- x: The independent variable (or predictor variable) — the input or feature used to make predictions.\n",
        "- \\beta_0: The intercept — the value of y when x = 0. It represents the baseline level of the response variable.\n",
        "- \\beta_1: The slope coefficient — it shows how much y changes for a one-unit increase in x. It captures the strength and direction of the relationship between x and y.\n",
        "- \\varepsilon: The error term — accounts for the variability in y that cannot be explained by the linear relationship with x. It represents random noise or other influencing factors not included in the model.\n",
        "Would you like to see how this works with a real-world example or dataset?\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q4. Provide a real-world example where simple linear regression can be\n",
        "applied.\n",
        "- Scenario:\n",
        "A real estate analyst wants to predict the price of a house based on its size (in square feet).\n",
        "Application of Simple Linear Regression:\n",
        "- Dependent Variable (y): House price\n",
        "- Independent Variable (x): Size of the house in square feet\n",
        "The analyst collects data on recent house sales, including the size and sale price of each home. Using simple linear regression, they fit a model:\n",
        "\\text{Price} = \\beta_0 + \\beta_1 \\cdot \\text{Size} + \\varepsilon\n",
        "This model helps estimate how much the price increases for each additional square foot. For example, if \\beta_1 = 1500, then each extra square foot adds ₹1500 to the expected price.\n",
        "\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q5.What is the method of least squares in linear regression?  \n",
        "- The method of least squares is a fundamental approach used in linear regression to find the best-fitting line through a set of data points. Its goal is to minimize the sum of the squared differences (called residuals) between the observed values and the values predicted by the linear model.\n",
        "\n",
        "Given a simple linear regression model:\n",
        "y = \\beta_0 + \\beta_1 x + \\varepsilon\n",
        "The method of least squares estimates the coefficients \\beta_0 (intercept) and \\beta_1 (slope) by minimizing the sum of squared residuals:\n",
        "\\text{Minimize } \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n",
        "Where:\n",
        "- y_i is the actual observed value\n",
        "- \\hat{y}_i is the predicted value from the model\n",
        "- x_i is the input value\n",
        "- n is the number of data points\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q6. What is Logistic Regression? How does it differ from Linear Regression?  \n",
        "- Logistic Regression is a statistical method used for binary classification — that is, predicting one of two possible outcomes (like yes/no, 0/1, true/false).\n",
        "\n",
        "Logistic regression is a statistical technique used for predicting binary outcomes, such as yes/no or success/failure, by modeling the probability that a given input belongs to a particular category. Unlike linear regression, which estimates a continuous numeric value using a straight-line relationship between variables, logistic regression uses the logistic (sigmoid) function to constrain its output between 0 and 1, representing probabilities. While linear regression is suitable for tasks like predicting house prices or temperatures, logistic regression is ideal for classification problems like spam detection or disease diagnosis. Another key difference lies in their loss functions: linear regression minimizes mean squared error, whereas logistic regression minimizes log loss (cross-entropy), making it better suited for handling categorical data.\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q7. Name and briefly describe three common evaluation metrics for regression\n",
        "models.\n",
        "  \n",
        "- Three common evaluation metrics for regression models are Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared. MAE measures the average magnitude of errors in predictions, without considering their direction, making it easy to interpret and less sensitive to outliers. MSE, on the other hand, squares the errors before averaging, which penalizes larger errors more heavily and is useful when large deviations are particularly undesirable. R-squared, also known as the coefficient of determination, indicates the proportion of variance in the dependent variable that is predictable from the independent variable, with values closer to 1 signifying a better fit. These metrics help assess how well a regression model performs and guide improvements\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q8. What is the purpose of the R-squared metric in regression analysis?\n",
        "  \n",
        "- The purpose of the R-squared metric in regression analysis is to measure how well the independent variable(s) explain the variability of the dependent variable. It represents the proportion of the total variation in the outcome that is accounted for by the regression model. R-squared values range from 0 to 1, where a value closer to 1 indicates that the model explains a large portion of the variance, and a value near 0 suggests that the model does not explain much of the variation. In essence, R-squared helps assess the goodness of fit of a regression model, showing how effectively the model captures the underlying data patterns.\n",
        "\n",
        "-----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9 Write Python code to fit a simple linear regression model using scikit-learn\n",
        "# and print the slope and intercept.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data: X = independent variable, y = dependent variable\n",
        "X = np.array([[1], [2], [3], [4], [5]])  # Feature values\n",
        "y = np.array([2, 4, 5, 4, 5])            # Target values\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the slope (coefficient) and intercept\n",
        "print(\"Slope (β1):\", model.coef_[0])\n",
        "print(\"Intercept (β0):\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeZXUyW7fv0G",
        "outputId": "fdb363f0-d3af-4a99-919b-1613d11bca86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (β1): 0.6\n",
            "Intercept (β0): 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10. How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "-In a simple linear regression model, the coefficients represent the relationship between the independent variable and the dependent variable. The intercept (\\beta_0) indicates the expected value of the dependent variable when the independent variable is zero — essentially, it's the baseline level of the outcome. The slope (\\beta_1) shows how much the dependent variable is expected to change for each one-unit increase in the independent variable. A positive slope means the variables move in the same direction (as x increases, y increases), while a negative slope indicates an inverse relationship (as x increases, y decreases). Together, these coefficients define the best-fit line that models the data.\n"
      ],
      "metadata": {
        "id": "BSq9eeFlgH9o"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}